{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sqlite3\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating     timestamp\n",
      "0     1.0     31.0     2.5  1.260759e+09\n",
      "1     1.0   1029.0     3.0  1.260759e+09\n",
      "2     1.0   1061.0     3.0  1.260759e+09\n",
      "3     1.0   1129.0     2.0  1.260759e+09\n",
      "4     1.0   1172.0     4.0  1.260759e+09\n",
      "(100004, 4)\n",
      "userId       float64\n",
      "movieId      float64\n",
      "rating       float64\n",
      "timestamp    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# path to the data directory\n",
    "data_path = '/mnt/DataSets/MovieLens/ml-latest-small'\n",
    "\n",
    "# read ratings and links into dataframes\n",
    "df_ratings = pd.read_csv(os.path.join(data_path,'ratings.csv'), sep=',')         \n",
    "df_ratings = df_ratings.astype(float) # this, for some reason, is very important!!\n",
    "\n",
    "print (df_ratings.head(5))\n",
    "print (df_ratings.shape)\n",
    "\n",
    "print (df_ratings.dtypes)\n",
    "#ls = list(df_ratings.head(5).to_records(index=False))\n",
    "#print(ls[0])\n",
    "#ls[0][0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 31, 2.5, 1260759144)\n",
      "(1, 1029, 3.0, 1260759179)\n",
      "(1, 1061, 3.0, 1260759182)\n",
      "(1, 1129, 2.0, 1260759185)\n",
      "(1, 1172, 4.0, 1260759205)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UserId         int64\n",
       "MovieId        int64\n",
       "Rating       float64\n",
       "timestamp      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create your connection.\n",
    "con = sqlite3.connect(':memory:')\n",
    "\n",
    "cur = con.cursor()\n",
    "cur.execute(\"CREATE TABLE Ratings (UserId INTEGER, MovieId INTEGER, Rating FLOAT, timestamp DATE);\")\n",
    "\n",
    "cur.executemany(\"INSERT INTO Ratings (UserId, MovieId, Rating, timestamp) VALUES(?,?,?,?)\", \n",
    "                list(df_ratings[['userId', 'movieId', 'rating', 'timestamp']].to_records(index=False)))\n",
    "\n",
    "query =\"SELECT * from Ratings Limit 5\"\n",
    "cur.execute(query)\n",
    "rows= cur.fetchall()\n",
    "for row in rows:\n",
    "    print (row)\n",
    "\n",
    "df_out = pd.read_sql_query(\"SELECT * from Ratings Limit 5\",con)\n",
    "df_out.dtypes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "for row in df_ratings.itertuples():\n",
    "    format_str = \"\"\"INSERT INTO ratings (UserId, MovieId, Rating, timestamp) \n",
    "                    VALUES (\"{UserId}\", \"{MovieId}\", \"{rating}\", \"{timestamp}\");\"\"\"\n",
    "    sql_command = format_str.format(UserId=row[1], MovieId=row[2], rating=row[3], timestamp = row[4])\n",
    "    cur.execute(sql_command)\n",
    "  \n",
    "query =\"\"\"SELECT * from Ratings Limit 5\"\"\"\n",
    "cur.execute(query)\n",
    "rows= cur.fetchall()\n",
    "for row in rows:\n",
    "    print (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- It took 31.319721937179565 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# 1. compute item-item co-occurrences thresholded\n",
    "cur.execute(\"CREATE TABLE cooc AS select a.MovieId as ItemId1, b.MovieId as ItemId2, count(*) as cooc \\\n",
    "            from ratings as a, ratings as b \\\n",
    "            where a.UserId = b.UserId and a.MovieId > b.MovieId \\\n",
    "            group by ItemId1, ItemId2 \\\n",
    "            having cooc >= 3\")\n",
    "\n",
    "# 2. compute item occurrences\n",
    "cur.execute(\"CREATE TABLE item_counts AS select MovieId, count(*) as occur from ratings group by MovieId order by occur desc;\")\n",
    "\n",
    "#3. compute item-item jaccard similarity\n",
    "cur.execute(\"CREATE TABLE jaccard AS select t1.ItemId1, t1.ItemId2, t1.cooc, \\\n",
    "           t2.occur as occur1, t3.occur as occur2, \\\n",
    "           (1.0*t1.cooc/(t2.occur+t3.occur-t1.cooc)) as jaccard \\\n",
    "            from item_counts t2 inner join cooc t1 on t1.ItemId1 = t2.MovieId inner join item_counts t3 on t1.ItemId2 = t3.MovieId \\\n",
    "            order by t1.ItemId1, jaccard desc;\")\n",
    "\n",
    "#4. convert item similarities to df\n",
    "df_jaccard = pd.read_sql_query(\"SELECT * from jaccard\", con)\n",
    "df_mpi = pd.read_sql_query(\"SELECT MovieId as ItemId, occur \\\n",
    "                                    from item_counts \\\n",
    "                                    order by occur desc \\\n",
    "                                    limit 10\", con)\n",
    "#5. clean up teh sql tables and close the connection\n",
    "cur.execute(\"DROP TABLE cooc\")\n",
    "cur.execute(\"DROP TABLE item_counts\")\n",
    "cur.execute(\"DROP TABLE jaccard\")\n",
    "\n",
    "#con.commit()\n",
    "con.close()\n",
    "\n",
    "print(\"--- It took %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "cur.execute(\"SELECT * FROM jaccard LIMIT 5\")\n",
    "rows= cur.fetchall()\n",
    "for row in rows:\n",
    "    print (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Item-Item Similarties---------------\n",
      "(2208878, 6)\n",
      "   ItemId1  ItemId2  cooc  occur1  occur2   jaccard\n",
      "0        2        1    71     107     247  0.250883\n",
      "1        3        1    39      59     247  0.146067\n",
      "2        3        2    18      59     107  0.121622\n",
      "3        4        3     5      13      59  0.074627\n",
      "4        4        2     5      13     107  0.043478\n",
      "ItemId1      int64\n",
      "ItemId2      int64\n",
      "cooc         int64\n",
      "occur1       int64\n",
      "occur2       int64\n",
      "jaccard    float64\n",
      "dtype: object\n",
      "Number of Unique Items: 4792\n",
      "----------Most Popular Items---------------\n",
      "   ItemId  occur\n",
      "0     356    341\n",
      "1     296    324\n",
      "2     318    311\n",
      "3     593    304\n",
      "4     260    291\n"
     ]
    }
   ],
   "source": [
    "print(\"----------Item-Item Similarties---------------\")\n",
    "print(df_jaccard.shape)\n",
    "print(df_jaccard.head(5))\n",
    "print(df_jaccard.dtypes)\n",
    "num_items = pd.concat([df_jaccard.ItemId1, df_jaccard.ItemId2],axis = 0).unique().shape[0]\n",
    "print(\"Number of Unique Items:\",num_items)\n",
    "print(\"----------Most Popular Items---------------\")\n",
    "print(df_mpi.head(5))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "def SAR_Score(simDF=None, usageDF=None, topN=5, actuals=False, fill=True, history=False, sim_idx=5):\n",
    "    \n",
    "    actuals = actuals\n",
    "    fill = fill\n",
    "    history = history\n",
    "    outputLS = []\n",
    "    \n",
    "    numItems = pd.concat([simDF.ItemId1, simDF.ItemId2],axis = 0).unique().shape[0]\n",
    "    print(\"DEBUG: There are %s unique items.\\n\"%(numItems))\n",
    "    \n",
    "    # initialize a dictionary/hash to map item ids to consecutive ints starting at 0\n",
    "    ilist={}\n",
    "    iIndex=0\n",
    "    # initialize a 2-D array to store item-item co-occurence counts and a 1-D to store Item occurrences\n",
    "    simMatrix = np.zeros(numItems*numItems).reshape(numItems,numItems)\n",
    "    totalOccur = np.zeros(numItems) \n",
    "    # iterate over the simDF and populate non-zero values in \n",
    "    for row in simDF.itertuples():\n",
    "        Item1 = str(row[1])\n",
    "        Item2 = str(row[2])     \n",
    "        #if(iIndex == 0):\n",
    "        #    print(row[1],row[2],row[6])\n",
    "        try:\n",
    "            # check if iid was added to ilist dictionary\n",
    "            ilist[Item1]\n",
    "        except KeyError:\t\n",
    "            # if iid not in ilist, add iid as key and consecutive int iIndex as value\n",
    "            ilist[Item1]=iIndex  \n",
    "            iIndex+=1\n",
    "        try:\n",
    "            # check if iid was added to ilist dictionary\n",
    "            ilist[Item2]\n",
    "        except KeyError:\t\n",
    "            # if iid not in ilist, add iid as key and consecutive int iIndex as value\n",
    "            ilist[Item2]=iIndex  \n",
    "            iIndex+=1\n",
    "        \n",
    "        totalOccur[ilist[Item1]] = row[4]\n",
    "        totalOccur[ilist[Item2]] = row[5]\n",
    "        \n",
    "        simMatrix[ilist[Item1]][ilist[Item2]] = simMatrix[ilist[Item2]][ilist[Item1]] = row[6]\n",
    "    \n",
    "    # get the opposite of ilist, where key = mapped itemid, value = original itemid        \n",
    "    rItem = {}\n",
    "    for k,v in ilist.items():\n",
    "        iid = k\n",
    "        mapIID = v\n",
    "        rItem[mapIID] = iid\n",
    "        \n",
    "    # store topN most popular items                     \n",
    "    topNItems=np.argsort(totalOccur)[::-1][:topN]\n",
    "    print(\"DEBUG: Most Popular Items:\")\n",
    "    for j in topNItems:\n",
    "        print(\"DEBUG: %s\\t%d\"%(rItem[j],totalOccur[j]))\n",
    "\n",
    "    sparsity = float(len(simMatrix.nonzero()[0]))\n",
    "    sparsity /= (simMatrix.shape[0] * simMatrix.shape[1])\n",
    "    sparsity *= 100\n",
    "    print (\"\\nDEBUG: Sparsity of Item Similarity Matrix%: \",sparsity)\n",
    " \n",
    "    # now, process the seed file to score, one user block at a time\n",
    "    numUsers = usageDF.UserId.unique().shape[0]\n",
    "    print(\"\\nDEBUG: There are %s users to score.\\n\"%(numUsers))\n",
    "    # a list to store all items rated and an array to store values\n",
    "    itemSet = []\n",
    "    itemValues = np.zeros(numItems)\n",
    "    pre_uid=\"\"\n",
    "    # reads from the input three-column file one line at a time, sorted by userId blocks\n",
    "    for row in usageDF.itertuples():\n",
    "        uid = str(row[1])\n",
    "        iid = str(row[2])\n",
    "        value = float(row[3])\n",
    "        if(value <= 0):\n",
    "            continue\n",
    "        # for a new \"user block\" start except the very first one\n",
    "        if (pre_uid != uid and pre_uid != \"\"): \n",
    "            if(len(itemSet) > 0):\n",
    "                rec = (itemSet,itemValues)\n",
    "                scoretop(simMatrix,ilist,rItem,rec,pre_uid,topNItems,outputLS,actuals,fill,history)\n",
    "            elif(fill): # backfill 0 reco users\n",
    "                n = 0\n",
    "                for j in topNItems:\n",
    "                    if(n < topN):\n",
    "                        outputLS.append((pre_uid,rItem[j],0.01,\"T\"))\n",
    "                        n +=1\n",
    "                    else:\n",
    "                        break   \n",
    "            \n",
    "            itemSet = []\n",
    "            itemValues = np.zeros(numItems)\n",
    "           \n",
    "        pre_uid=uid\n",
    "        if(iid in ilist):\n",
    "            itemSet.append(iid) # append raw iid to list itemSet\n",
    "            itemValues[ilist[iid]] = value\n",
    "  \n",
    "    if(len(itemSet) > 0):\n",
    "        rec = (itemSet,itemValues)\n",
    "        scoretop(simMatrix,ilist,rItem,rec,pre_uid,topNItems,outputLS,actuals,fill,history)\n",
    "    elif(fill): # backfill 0 reco users\n",
    "        n = 0\n",
    "        for j in topNItems:\n",
    "            if(n < topN):\n",
    "                outputLS.append((pre_uid,rItem[j],0.01,\"T\"))\n",
    "                n +=1       \n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    outputDF = pd.DataFrame(outputLS)\n",
    "    outputDF.columns = ['UserId', 'ItemId', 'Score', 'Flag']\n",
    "  \n",
    "    return outputDF\n",
    "        \n",
    "\n",
    "    # SAR Scoring user for affinity to items no reasons\n",
    "def scoretop(itemCoocs,ilist,rItem,rec,uid,topNItems,outputLS,actuals,fill,history):\n",
    "    #print rec\n",
    "    (ratedItems,ratedValues) = rec\n",
    "    # if ratedItems is a dict, sort by value desc\n",
    "    #ratedItemsS = sorted(ratedItems, key=ratedItems.get, reverse=True)\n",
    "\n",
    "    # print the historical for this user first\n",
    "    recosItemsHash = {}\n",
    "    ratedItemsHash = {}\n",
    "    for iid in ratedItems:\n",
    "        ratedItemsHash[iid] = 1\n",
    "        #print \"%s\\t%s\\t%.1f\\tH\" % (uid,iid,ratedValues[ilist[iid]])\n",
    "        if history:\n",
    "            outputLS.append((uid,iid,ratedValues[ilist[iid]],\"H\"))\n",
    "  \n",
    "    # multiply the co-occur matrix with the user vector ratedValues\n",
    "    numItems = len(ilist)\n",
    "    simValues = np.zeros(numItems)\n",
    "    #simValues = np.dot(itemCoocs,ratedValues) # this is an expensive operation, use below instead\n",
    "    nonzero = np.nonzero(ratedValues)\n",
    "    for i in nonzero[0]:\n",
    "        temp = np.multiply(ratedValues[i],itemCoocs[ : ,i])\n",
    "        simValues += temp\n",
    "    \n",
    "    topN = len(topNItems)\n",
    "    top = topN + len(ratedItemsHash)\n",
    "    indv = []\n",
    "    #indv=np.argsort(simValues)\n",
    "    indv=np.argsort(simValues)[::-1][:top]\n",
    "    cnt=0\n",
    "    for i in indv:\n",
    "        if(cnt==0):\n",
    "            max_scr = simValues[i] + 0.1\n",
    "        iid = rItem[i]\n",
    "        if(not actuals and iid in ratedItemsHash):  \n",
    "            pass\n",
    "        else: # \n",
    "            if(cnt < topN):  # restrict to topN highest scores\n",
    "                scr = simValues[i]\n",
    "                if(scr > 0):\n",
    "                    scr = scr/max_scr\n",
    "                    if(actuals and iid in ratedItemsHash):\n",
    "                        #print \"%s\\t%s\\t%.4f\\tA\" % (uid,iid,scr)\n",
    "                        outputLS.append((uid,iid,scr,\"A\"))\n",
    "                    else:\n",
    "                        #print \"%s\\t%s\\t%.4f\\tR\" % (uid,iid,scr)\n",
    "                        outputLS.append((uid,iid,scr,\"R\"))\n",
    "                        recosItemsHash[iid] = 1\n",
    "                elif(fill): # backfill with topN items across population\n",
    "                    for j in topNItems:\n",
    "                        if(not (rItem[j] in ratedItemsHash) and not (rItem[j] in recosItemsHash)):\n",
    "                            #print \"%s\\t%s\\t%.4f\\tT\" % (uid,rItem[j],0.01)\n",
    "                            outputLS.append((uid,rItem[j],0.01,\"T\"))\n",
    "                            recosItemsHash[rItem[j]] = 1\n",
    "                            break\n",
    "                cnt+=1\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: There are 4792 unique items.\n",
      "\n",
      "DEBUG: Most Popular Items:\n",
      "DEBUG: 356\t341\n",
      "DEBUG: 296\t324\n",
      "DEBUG: 318\t311\n",
      "DEBUG: 593\t304\n",
      "DEBUG: 260\t291\n",
      "\n",
      "DEBUG: Sparsity of Item Similarity Matrix%:  19.238362629981523\n",
      "\n",
      "DEBUG: There are 671 users to score.\n",
      "\n",
      "  UserId ItemId     Score Flag\n",
      "0      1   2105  0.802469    R\n",
      "1      1   3108  0.573477    R\n",
      "2      1   2872  0.558239    R\n",
      "3      1   1129  0.556425    R\n",
      "4      1   2021  0.554473    R\n",
      "\n",
      "Freq of Flag:\n",
      " R    3021\n",
      "T     334\n",
      "Name: Flag, dtype: int64\n",
      "\n",
      "--- It took 11.551105260848999 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from SAR_scoring import SAR_Score\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# read in the seed file\n",
    "df_seeds = pd.read_csv(os.path.join(data_path,'SecondToLastItemPerUser.csv'), sep=',', \n",
    "                       header = None, names=['UserId','ItemId','Aff'])\n",
    "\n",
    "df_scores = SAR_Score(simDF = df_jaccard, usageDF = df_seeds)   \n",
    "print(df_scores.head(5))\n",
    "print(\"\\nFreq of Flag:\\n\",df_scores.Flag.value_counts())\n",
    "print(\"\\n--- It took %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert the <UserId, ItemId, prediction> format to horizontal: <UserId, Reco1, Reco2, ..., Reco5>\n",
    "k_max = 5\n",
    "D_recos = []\n",
    "pre = \"\"\n",
    "sim_items = []\n",
    "k = 1\n",
    "for row in df_scores.itertuples():\n",
    "    UserId = str(row[1])\n",
    "    ItemId = str(row[2])\n",
    "    \n",
    "    # for a new user block\" start except the very first one\n",
    "    if (pre == \"\"):\n",
    "        sim_items.append(UserId)\n",
    "    if (pre != UserId and pre != \"\"): \n",
    "        D_recos.append(sim_items)\n",
    "        sim_items = []\n",
    "        sim_items.append(UserId) \n",
    "        k = 1\n",
    "    if(k <= k_max):\n",
    "        pre=UserId   \n",
    "        sim_items.append(ItemId)\n",
    "        k +=1\n",
    "    else:\n",
    "        pass\n",
    "# last item related items\n",
    "D_recos.append(sim_items)\n",
    "\n",
    "df_recos_h = pd.DataFrame(D_recos, dtype='str')\n",
    "df_recos_h.to_csv(data_path + '/pred_horizontal_SAR.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1\n",
      "precision_avg = 0.022355\n",
      "counted user number = 671\n",
      "k=2\n",
      "precision_avg = 0.032787\n",
      "counted user number = 671\n",
      "k=3\n",
      "precision_avg = 0.041729\n",
      "counted user number = 671\n",
      "k=4\n",
      "precision_avg = 0.053651\n",
      "counted user number = 671\n",
      "k=5\n",
      "precision_avg = 0.061103\n",
      "counted user number = 671\n",
      "1210\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /mnt/DataSets/MovieLens/ml-latest-small\n",
    "\n",
    "# Compute Precision@1-5 on out of time sample Test file with an external python script\n",
    "for k in 1 2 3 4 5; do\n",
    "    /home/vediev/bin/recPrecision_Arg.py -t lastItemPerUser.csv -p pred_horizontal_SAR.csv -k ${k}\n",
    "done\n",
    "\n",
    "cat pred_horizontal_SAR.csv | cut -d, -f2- | tr \",\" \"\\n\" | sort | uniq | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
